{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning : Assignment"
      ],
      "metadata": {
        "id": "vkZAMJzv332X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Ans.  Ensemble learning in machine learning is a technique where we combine multiple models (often called weak learners or base learners) to produce a stronger and more accurate final model.\n",
        "\n",
        "Key Idea:\n",
        "\n",
        "The central idea is:\n",
        "\n",
        "A group of models, when combined, can often perform better than any single model alone.\n",
        "\n",
        "Just like asking several experts instead of relying on one person often gives a more reliable decision, ensemble learning aggregates predictions from different models to reduce errors, bias, and variance.\n",
        "\n",
        "Why it works\n",
        "\n",
        "Reduction of variance – Different models may make different mistakes; averaging them smooths out random errors.\n",
        "\n",
        "Reduction of bias – Combining models can capture more complex patterns.\n",
        "\n",
        "Better generalization – More robust performance on unseen data.\n",
        "\n",
        "\n",
        "2.What is the difference between Bagging and Boosting?\n",
        "\n",
        "Ans. Bagging (Bootstrap Aggregating) and Boosting are both ensemble learning techniques, but they work in very different ways.\n",
        "In Bagging, multiple models are trained in parallel, each on a different random subset of the training data created through bootstrap sampling (sampling with replacement). These models are independent of each other, and their predictions are later combined, usually by majority voting for classification or averaging for regression. The main goal of Bagging is to reduce variance and make the final prediction more stable; Random Forest is a well-known example.\n",
        "\n",
        "Boosting, on the other hand, trains models sequentially, where each new model focuses on correcting the mistakes made by the previous ones. In this process, data points that were misclassified earlier are given more weight so that subsequent models pay more attention to them. Boosting primarily aims to reduce bias and improve accuracy, but it can be prone to overfitting if overtrained. Popular Boosting algorithms include AdaBoost, Gradient Boosting, XGBoost, and LightGBM.\n",
        "\n",
        "In short, Bagging builds many independent models to average out their errors, while Boosting builds dependent models that learn from each other’s mistakes to progressively improve performance.\n",
        "\n",
        "\n",
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "Ans. Bootstrap sampling is a statistical technique where we create new datasets by randomly sampling from the original dataset with replacement.\n",
        "This means a single data point from the original dataset can appear multiple times in a bootstrap sample, while some points might not appear at all.\n",
        "\n",
        "How it works in Bagging\n",
        "\n",
        "In Bagging methods like Random Forest, bootstrap sampling is used to give each model (or decision tree) a slightly different view of the data:\n",
        "\n",
        "From the original dataset of size N, we randomly pick N samples with replacement to form a new dataset.\n",
        "\n",
        "This process is repeated for each model in the ensemble, so every model is trained on a different bootstrap sample.\n",
        "\n",
        "Because each model sees a different subset, they learn different patterns and make different errors.\n",
        "\n",
        "Finally, their predictions are combined (by averaging or majority voting), which reduces variance and improves stability.\n",
        "\n",
        "\n",
        "Key Role in Bagging:\n",
        "\n",
        "Introduces diversity among models.\n",
        "\n",
        "Prevents all models from making the same mistakes.\n",
        "\n",
        "Helps reduce overfitting compared to using the entire dataset for each model.\n",
        "\n",
        "\n",
        "4.What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Ans.  Out-of-Bag (OOB) samples are the data points not included in a bootstrap sample when building a model in Bagging methods like Random Forest.\n",
        "\n",
        "Since bootstrap sampling is done with replacement, on average about 63% of the original dataset is selected for training each model, and the remaining ~37% is left out — these are the OOB samples.\n",
        "\n",
        "How OOB Samples Are Used\n",
        "\n",
        "For each model (e.g., each decision tree in a Random Forest), the OOB samples serve as a built-in validation set.\n",
        "\n",
        "After training a model on its bootstrap sample, we test it on its OOB samples to measure performance without needing a separate validation dataset.\n",
        "\n",
        "OOB Score\n",
        "\n",
        "The OOB score is the average prediction accuracy (or another metric) computed across all OOB samples for the entire ensemble:\n",
        "\n",
        "For each data point in the original dataset, only consider predictions from the models where that point was OOB.\n",
        "\n",
        "Compare these predictions to the actual labels.\n",
        "\n",
        "Average the results to get the OOB score.\n",
        "\n",
        "Benefits of OOB Evaluation:\n",
        "\n",
        "No need for a separate test/validation set (saves data).\n",
        "\n",
        "Provides an unbiased estimate of model performance during training.\n",
        "\n",
        "Especially useful when data is limited.\n",
        "\n",
        "\n",
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "Ans.  In a single Decision Tree, feature importance is measured based on how much each feature contributes to reducing impurity (like Gini impurity or entropy for classification, and variance for regression) when it’s used to split the data. For each split in the tree, the algorithm calculates the decrease in impurity, and these decreases are summed over all nodes where the feature appears. The higher the total decrease, the more “important” the feature is considered. However, because a single decision tree is sensitive to the specific training data, its feature importance can be unstable — if you train the tree on slightly different data, the importances might change a lot.\n",
        "\n",
        "In a Random Forest, feature importance is calculated in a similar way, but it’s averaged over many trees. Each tree is built on a bootstrap sample and uses random feature subsets for splitting, so the importance score for a feature comes from summing impurity decreases across all trees and then averaging. This aggregation makes the importance scores more robust and reliable than in a single tree, since the effect of noise or peculiarities in one dataset split is reduced. Random Forests can also use permutation importance, which measures how much the model’s accuracy drops when a feature’s values are randomly shuffled — this method captures both linear and non-linear dependencies.\n",
        "\n",
        "In short:\n",
        "\n",
        "Decision Tree → Importance from impurity reduction in one tree, often unstable.\n",
        "\n",
        "Random Forest → Importance from impurity reduction averaged over many trees, more stable and generalizable.\n",
        "\n",
        "6.Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "uBAkJrNF4Bil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# 2. Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# 3. Get feature importance scores\n",
        "feature_importances = pd.Series(model.feature_importances_, index=data.feature_names)\n",
        "\n",
        "# 4. Sort and display top 5 features\n",
        "top_features = feature_importances.sort_values(ascending=False).head(5)\n",
        "print(\"Top 5 Most Important Features:\\n\")\n",
        "print(top_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0dMFLTQ6XtQ",
        "outputId": "da4e3a8c-78e2-44c4-d4c6-27eaca85568d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree."
      ],
      "metadata": {
        "id": "rYncnYZu6pav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1) Data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 2) Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "# 3) Bagging with Decision Trees (handle sklearn version differences)\n",
        "common = dict(n_estimators=50, random_state=42)\n",
        "try:\n",
        "    # scikit-learn >= 1.2\n",
        "    bag = BaggingClassifier(estimator=DecisionTreeClassifier(), **common)\n",
        "except TypeError:\n",
        "    # scikit-learn < 1.2\n",
        "    bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(), **common)\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
        "\n",
        "# 4) Compare\n",
        "print(f\"Single Decision Tree Accuracy: {dt_acc:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy  : {bag_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yigAc2Y57XZn",
        "outputId": "a584e53e-df03-4b61-a247-daf826177d89"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 0.9333\n",
            "Bagging Classifier Accuracy  : 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "TiZfLG6k7fzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Define model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 4. Define parameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 150],\n",
        "    \"max_depth\": [None, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# 5. GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 7. Final accuracy with best params\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Final Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ8oGcoJ7iGY",
        "outputId": "0eb60676-6668-40f0-e64d-dc11b3192395"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 0.9357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "_-0MQoVu7xk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Bagging Regressor (using Decision Trees as base estimators)\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# 4. Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions\n",
        "bagging_pred = bagging_reg.predict(X_test)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "# 6. Calculate MSE\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# 7. Compare results\n",
        "print(f\"Bagging Regressor MSE      : {bagging_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm_6tSBH73Mw",
        "outputId": "003c12e6-3707-40a6-994f-3a6417f798b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE      : 0.2579\n",
            "Random Forest Regressor MSE: 0.2577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "\n",
        "Ans. 1. Choosing Between Bagging or Boosting\n",
        "\n",
        "Start with the problem nature:\n",
        "\n",
        "Loan default prediction is a classification problem, often with class imbalance (fewer defaults than non-defaults).\n",
        "\n",
        "Boosting (e.g., XGBoost, LightGBM) is generally better when you want higher accuracy and can tolerate more complexity, because it reduces bias and focuses on hard-to-classify cases.\n",
        "\n",
        "Bagging (e.g., Random Forest) is better when the dataset is noisy and you want stability by reducing variance.\n",
        "\n",
        "Decision:\n",
        "I’d try Boosting first because financial risk models benefit from focusing on edge cases (e.g., borderline defaulters). But I’d also benchmark against Bagging to see if it performs comparably without overfitting.\n",
        "\n",
        "2. Handling Overfitting\n",
        "\n",
        "For Bagging (Random Forest):\n",
        "\n",
        "Limit max_depth of trees.\n",
        "\n",
        "Increase min_samples_split or min_samples_leaf.\n",
        "\n",
        "Use fewer features per split (max_features).\n",
        "\n",
        "For Boosting:\n",
        "\n",
        "Use smaller learning rates (learning_rate).\n",
        "\n",
        "Limit number of boosting rounds (n_estimators).\n",
        "\n",
        "Use early stopping with validation data.\n",
        "\n",
        "Apply regularization (max_depth, min_child_weight, lambda, alpha).\n",
        "\n",
        "Data-related methods:\n",
        "\n",
        "Remove irrelevant features.\n",
        "\n",
        "Use feature selection to reduce noise.\n",
        "\n",
        "Apply SMOTE or class weights to balance target classes.\n",
        "\n",
        "3. Selecting Base Models\n",
        "\n",
        "Bagging: Decision Trees are a natural fit (Random Forest).\n",
        "\n",
        "Boosting: Decision Trees are also common (e.g., Gradient Boosted Trees), but you can also use shallow trees for better generalization.\n",
        "\n",
        "I’d start with tree-based models because:\n",
        "\n",
        "They handle mixed data types well.\n",
        "\n",
        "They capture non-linear patterns and interactions between variables without heavy preprocessing.\n",
        "\n",
        "If using Stacking later, I might include Logistic Regression or LightGBM in the base layer for diversity.\n",
        "\n",
        "4. Evaluating Performance with Cross-Validation\n",
        "\n",
        "Use Stratified k-Fold Cross-Validation to preserve the proportion of defaulters vs. non-defaulters in each fold.\n",
        "\n",
        "Evaluate using metrics beyond accuracy:\n",
        "\n",
        "AUC-ROC → measures ranking ability (important for loan risk).\n",
        "\n",
        "Precision-Recall AUC → especially useful for imbalanced datasets.\n",
        "\n",
        "F1-score → balances precision and recall.\n",
        "\n",
        "Keep a validation curve to see how performance changes with n_estimators, depth, etc., to detect overfitting early.\n",
        "\n",
        "5. Justifying Ensemble Learning in This Real-World Context\n",
        "\n",
        "Why it works here:\n",
        "\n",
        "Loan default prediction has complex patterns — a single model might miss subtle risk signals.\n",
        "\n",
        "Ensembles combine multiple models’ strengths, reducing the chance of relying on one biased viewpoint.\n",
        "\n",
        "Business value:\n",
        "\n",
        "Lower default rates → more accurate classification of high-risk customers means better loan approval decisions.\n",
        "\n",
        "Optimized profit → approving safe borrowers while rejecting risky ones improves financial returns.\n",
        "\n",
        "Regulatory compliance → robust models with better generalization lower the chance of biased or unstable decisions.\n",
        "\n",
        "Risk management → more consistent predictions reduce uncertainty in portfolio risk assessment."
      ],
      "metadata": {
        "id": "r_jvYDDd8LAQ"
      }
    }
  ]
}